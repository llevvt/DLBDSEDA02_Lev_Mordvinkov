RAW_DATASET='./complaints_processed.csv' #Path within the system to the raw dataset
NORMALIZED_TEXTS='./normalized_texts.csv' #Path within the system to the file with normalized dataset
OUTPUT_DIR=results
#----------- Normalization Settings
MIN_TOKEN_LEN='2'   #This parameter defines the minimal size of token, that should be saved after lemmatization.
                    #Bigger minimal token length - clearer the dictionary, but important short words can be lost
                    #Smaller minimal token length - more words available, but more noise (mistypes, short words)

#----------- Vectorization Settings (for both BoW and TF-IDF)
VOCABULARY_SIZE='20000'     #This parameter defines the number of terms, that should be included into vocabulary. The most frequent terms would be taken
                            #Bigger vocabulary - more detalized the model is, but learns slower
                            #Smaller vocabulary - less detalized, but learns faster
MIN_N_GRAM='1'  #This parameter defines the minimum number of words that should be included into n-grams.
MAX_N_GRAM='2'  #This parameter defines the maximum number of words that should be included into n-grams.
                #Bigger range - more features, more details, model will consider phrases, but learns slower
                #Smaller range - less features, context can be lost, but learns faster
MIN_FREQUENCY='2'   #This parameter defines the number of appearance of the word accross documents
                    #Bigger value - rare words are removed, model has less noise, learning is faster, but important rare lexic can be lost
                    #Smaller value - more detales, but more noise and model can "over-learn"

#----------- Topics Extraction Settings (for both LDA and LSA)
NUMBER_OF_TOPICS='10'   #This parameter regulates the number of topics, that should be detected within the corpus.
                        #More topics - they become more specific, but can produce noise or have close meaning to each othe.
                        #Less topics - more general and wide topics, some important differences can be lost

NUMBER_OF_TOP_WORDS='15'    #This parameter regulates the number of top words for each topic
                            #More top words - provides deeper understanding of the topic (more context), but can show less relevant words
                            #Less top words - less context, but the most relevant terms

NUMBER_OF_ITERATIONS='20'   #This parameter defines the nuber of iterations for Expectation-Maximization algorithm, which calculates the distribution of terms among topics
                            #More iterations - model learns longer, but it differentiate topics better and result is more consistent
                            #Less iterations - model learns faster, but the results can be less consisten (will differentiate from each run of the script)

RANDOM_STATE='42'   #Determines the seed for the randomizer. If not set, the model will return different result for each run.

#LDA-specific settings
NUMBER_OF_JOBS='-1'  #Determines the nuber of CPUs, which are used for learning. -1 - use all available CPUs. Affects the calculation time

